\documentclass{article}
\usepackage[fleqn]{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}

\usepackage{tikz}
\usetikzlibrary{arrows}

\usepackage{titlesec, hyperref}
\newcommand{\sectionbreak}{\clearpage}
\newcommand{\R}{\mathbb{R}}

\title{Model Identification and Data Analysis}
\author{Matteo Secco}

\begin{document}
\maketitle
\newpage
\tableofcontents
\newpage

\part{Prediction}
\section{Probability Recall}

\subsection{Random Vectors}

\paragraph{Variance}
$Var[v]=E[(v-E[v])^2]$
\paragraph{Cross-Variance}
$Var[v,u]=E[(v-E[v])(u-E[u])]$
\paragraph{Variance Matrix}
$\begin{vmatrix}
	Var[v_1]			&	.	&	.	&	Var[v_1,v_k]		\\
		.			&	.	&		&		.			\\
		.			&		&	.	&		.			\\
	Var[v_k,v_1]		&	.	&	.	&	Var[v_k]			\\
				
\end{vmatrix}$
\paragraph{Covariance coefficient}
	$\delta[i,j]=\frac{Var[i,j]}{\sqrt{Var[i]}\sqrt{Var[j]}}$\\
	$\delta[i,j]=0 \implies$ i, j uncorrelated\\
	$\left|\delta[i,j]\right|=1 \implies i=\alpha j$
	
\subsection{Random processes} $v(t,s) |$ t time instant, s expetiment outcome (generally given)

\paragraph{Mean} $m(t)=E[v(t,s)]$
\paragraph{Variance}
$\lambda^2 (t)=Var[v(t)]$ 
\paragraph{Covariance function}
$\gamma(t_1,t_2)=E[(v(t_1)-m(t_1))(v(t_2)-m(t_2))]=\gamma(t_2,t_1)$
\paragraph{Normalized Covariance Function}
$\rho(\tau)=\frac{\gamma(\tau)}{\gamma(0)}$\\
$\forall$ stationary processes: $|\rho(\tau)|\leq1	\quad\forall\tau$

\subsection{Important process classes}

\paragraph{Stationary process}
\begin{itemize}
	\item $m(t)=m$ constant
	\item $\lambda^2 (t)=\lambda^2$ constant
	\item $\gamma(t_1,t_2)=f(t_2-t_1)=\gamma(\tau)$ covariance depends only on time difference $\tau$
\end{itemize}
$|\gamma(\tau)|\leq\gamma(0)		\quad\forall\tau$
\paragraph{White noise} $\eta(t)\sim WN(m,\lambda^2)$
\begin{itemize}
	\item Stationary process
	\item $\gamma(\tau)=0	\quad\forall\tau\neq0$
\end{itemize}
$v(t)=\alpha \eta (t)+\beta	\quad 
\eta (t)\sim WN(0,\lambda^2)	
\qquad\implies\qquad 
v(t)\sim WN(\beta,\alpha^2 \lambda^2)$

\section{Spectral Analysis}

\subsection{Foundamentals}
\paragraph{Spectrum} \[
\Gamma(\omega)=\overbrace{F(\gamma(\tau))}^\text{Fourier transform}=\sum_{\tau=-\infty}^{+\infty}\gamma(\tau)\cdot e^{-j\omega\tau}
\]
\paragraph{Euler formula} $\Gamma(\omega)=\gamma(0)+2cos(\omega)\gamma(1)+2cos(2\omega)\gamma(2)+...$
\paragraph{Spectrum properties} 
\begin{itemize}
	\item $\Gamma: \R \to \R$
	\item $\Gamma\text{ is periodic with }T=2\pi$
	\item $\Gamma\text{ is even }[\Gamma(-\omega)=\Gamma(\omega)]$
	\item $\Gamma(\omega)\geq 0 \quad \forall\omega$
\end{itemize}
$\eta(t) \sim WN(0,\lambda^2) \quad\implies\quad \Gamma_\eta(\omega)=\gamma(0)=Var[\eta(t)]=\lambda^2$
\paragraph{Anti-Transform}
\[
\gamma(\tau)=\frac{1}{2\pi}\int_{-\pi}^{+\pi} \Gamma(\omega)e^{k\omega\tau}\,dw
\]
\paragraph{Complex spectrum}
\[
\phi(z)=\sum_{\tau =-\infty}^{+\infty} \omega(\tau)z^{-\tau}
\]
\[
\Gamma(\omega)=\Phi(e^{j\omega})
\]

\subsection{Fundamental theorem of Spectral Analysis} 

\paragraph{Fundamental theorem of Spectral Analysis} allows to derive the (real and/or complex) spectrum of the output from  the input and the transfer function of the system

\tikzstyle{block}=[draw, minimum size=3.5em]

\begin{tikzpicture}[node distance=5em,auto,>=latex']
    \node [block] (W) {$W(z)$};
    \node (u) [left of=W, coordinate] {};
    \node [coordinate] (y) [right of=W]{};
    \path[->] (u) edge node {$u$} (W);
    \path[->] (W) edge node {$y$} (y);
\end{tikzpicture}

\begin{alignat*}{2}
	\Gamma_{yy}(\omega)&=|W(e^{j\omega})|^2 & \cdot \Gamma_{uu}(\omega)\\
	\Phi_{yy}(z)&=W(z)W(z^{-1})				& \cdot \Phi_{uu}(z)
\end{alignat*}

\subsection{Canonical representation of a Stationary Process}
A stationary process can be represented by an infinite number of transfer functions. The canonical representation is the transfer function $W(z)$ such that:
\begin{itemize}
\item Numerator and denominator have \underline{same degree}
\item Numerator and denominator are \underline{monic} (highest grade coefficient is 1)
\item Numerator and denominator are \underline{coprime} ($W(z)$ cannot be simplified)
\item numerator and denominator are \underline{stable polynomials} (all poles and zeros of $W(z)$ are inside the unit disk)
\end{itemize}

\section{Moving Average Processes}
Given $\eta(t) \sim WN(0,\lambda^2)$
\subsection{MA(1): }
\paragraph{Model}
\[v(t)=c_0\eta(t)+c_1\eta(t-1)\]


\subparagraph{Mean}
\begin{alignat*}{2}
	E[v(t)]		&=	c_0\cdot E[\eta(t)]	&	&+	c_1\cdot E[\eta(t)]	\\
				&=	c_0\cdot 0			&	&+	c_1\cdot 0			\\
	\Aboxed{E[v(t)]&=0}
\end{alignat*}\\


\subparagraph{Variance}
\begin{alignat*}{3}
	Var[v(t)]	&=	E[(v(t)	\underbrace{-E[v(t)])^2}_{0}]													\\
				&=	E[(v(t))^2]						\\
				&=	E[(c_0\cdot	\eta(t)^2		&+&	c_1\cdot	\eta(t-1))^2]	\\
				&=	c_0^2\cdot 	E[\eta(t)^2]		&+&	c_1^2\cdot E[\eta(t-1)^2]	&+	\underbrace{2 c_0 c_1 \cdot E[\eta(t)\eta(t-1)]}_0\\
				&=	c_0^2\cdot 	E[\eta(t)^2]		&+&	c_1^2\cdot E[\eta(t-1)^2]\\
				&=	c_0^2\lambda^2				&+&	c_1^2\lambda^2\\
	\Aboxed{Var[v(t)]&=(c_0^2+c_1^2)\lambda^2}
\end{alignat*}


\subparagraph{Covariance}
\begin{alignat*}{4}
	\gamma(t_1,t_2)	&=	E[(v(t_1)-E[v(t_1)])				&\cdot 	&		(v(t_2)-E[v(t_2)])]\\
					&=	E[(c_0\eta(t_1)+c_1\eta(t_1-1))	&\cdot 	&		(c_0\eta(t_2)+c_1\eta(t_2-1))]\\
					&=	c_0^2E[\eta(t_1)\eta(t_2)]		&+&				c_1^2E[\eta(t_1-1)\eta(t_2-1)\\
					&									&+&				c_0c_1E[\eta(t_1)\eta(t_2-1)]
						+c_0c_1E[\eta(t_1-1)\eta(t_2)]
\end{alignat*}
\[
\boxed{
	\gamma(\tau)=\begin{cases}
		c_0^2\lambda^2+c_1^2\lambda^2	&\text{if }\tau=0\\
		c_0c_1\lambda^2					&\text{if }\tau=\pm 1\\
		0								&\text{otherwise}
	\end{cases}
}
\]

\subsection{MA(n)}

\paragraph{Model}
\begin{align*}
v(t)&=c_0\eta(t)+c_1\eta(t-1)+...+c_n\eta(t-n)\\
&=(c_0+c_1z^{-1}+...+c_nz^{-n})\eta(t)
\end{align*}

\paragraph{Transfer function}
\[W(z)
=c_0+c_1z^{-1}+...+c_nz^{-n}
=\frac{c_0z^n+c_1z^{n-1}+...+c_n}{z^n}
\]\\
All poles are in the complex origin

\paragraph{Mean}
\begin{alignat*}{2}
E[v(t)]&=(c_0+c_1+...+c_n)\underbrace{E[\eta(t)]}_0\\
\Aboxed{E[v(t)&=0}
\end{alignat*}

\paragraph{Covariance function}
\[
\boxed{
\gamma(\tau)=
\begin{cases}
\lambda^2\cdot \sum_{i=0}^{n-\tau} c_ic_{i-\tau}	&	|\tau|\leq n\\
0	& \text{otherwise}
\end{cases}
}
\]
\subparagraph{example}
\begin{align*}
\gamma(0)&=(c_0^2+c_1^2+...+c_n^2)\lambda^2\\
\gamma(1)&=(c_0c_1+c_1c_2+...+c_{n-1}c_n)\lambda^2\\
\gamma(2)&=(c_0c_2+c_1c_3+...+c_{n-2}c_n)\lambda^2\\
&...\\
\lambda(n)&=(c_0c_n)\lambda^2\\
\lambda(k)&=0\:\forall k>n
\end{align*}

\subsection{MA($\infty$)}
\paragraph{Model}
\[
v(t)=c_0\eta(t)+c_1\eta(t-1)+...+c_k\eta(t-k)+...=\sum_{i=0}^{\infty}c_i\eta(t-i)
\]
\paragraph{Variance}
\[
\gamma(0)
=(c_0^2+c_1^2+...+c_k^2+...) \lambda^2
=\lambda^2 \sum_{i=0}^{\infty} c_i^2
\]
\subsection{Well definition of an MA($\infty$)}
We need to have $|\gamma(\tau)|\leq \gamma(0)$, so we must require that \\
$\boxed{ \gamma(0)=\lambda^2 \sum_{i=0}^{\infty} c_i^2 \text{ is finite}}$

\section{Auto Regressive Processes}
\subsection{AR(1)}
\paragraph{Model}
\[v(t)=av(t-1)+\eta(t)\]
\paragraph{Mean}
\begin{alignat*}{2}
E[v(t)]&=E[av(t-1)]+\overbrace{E[\eta(t)]}^0\\
&=aE[v(t-1)]\\
&=aE[v(t)]\\
(1-a)E[v(t)]&=0\\
\Aboxed{
E[v(t)]&=0
}
\end{alignat*}
\paragraph{Covariance}
\subparagraph{MA($\infty$) method}
Observe as an AR(1) can  be axpressed as an MA($\infty$)
\begin{align*}
v(t)
&=av(t-1)	&+&		\eta(t)\\
&=a[av(t-2)+\eta(t-1)]	&+&	\eta(t)\\
&=a^2v(t-2)				&+&	a\eta(t-1) + \eta(t)\\
&=a^2[v(t-3)+\eta(t-2)]	&+& a\eta(t-1) + \eta(t)\\
&=\underbrace{a^nv(t-n)}_{\to 0}+ \underbrace{\sum_{i=0}^\infty a^i\eta(t-i)}_{\text{MA($\infty$)}}
\end{align*}
In particular, the result depends on an $MA(\infty)$ having $\sum_{i=0}^{\infty} c_i = \sum_{i=0}^{\infty} a^i$.\\
To check if the variance is finite we check $\gamma(0)=\lambda^2 \sum_{i=0}^{\infty} a^{2i}<\infty$. The given is a geometric series, convergent for $|a|<1$. Under this hypothesis its value is
\[
\gamma(0)=\lambda^2 \sum_{i=0}^{\infty} a^{2i}=\frac{\lambda^2}{1-a^2}
\] 
Applying the formula of the variance of MA processes we get\\
\hspace*{-1cm}\vbox{	%slightly move these equations to the right
 
 \begin{alignat*}{7}
\gamma(1)
	&=(c_0c_1+c_1c_2+...)\lambda^2
	&=(a+aa^2+...)\lambda^2
	&=a(1+a^2+a^4+...)\lambda^2
	&=a\lambda^2\sum_{i=0}^{\infty} a^{2i}
	&=a\frac{\lambda^2}{1-a^2}
	&=a\gamma(0)\\
\gamma(2)
	&=(c_0c_2+c_1c_3+...)\lambda^2
	&=(a^2+aa^3+...)\lambda^2
	&=a^2(1+a^2+a^4+...)\lambda^2
	&=a^2\lambda^2\sum_{i=0}^{\infty} a^{2i}
	&=a^2\frac{\lambda^2}{1-a^2}
	&=a^2\gamma(0)\\
\Aboxed{
\gamma(\tau)&=a^{|\tau|}\frac{\lambda^2}{1-a^2}
}
\end{alignat*}
} 
\subparagraph{Yule-Walkler Equations}
\begin{align*}
Var[v(t)]	
	&=E[v(t)^2]\\
	&=E[(av(t)+\eta(t))^2]\\
	&=a^2\underbrace{E[v(t-1)^2]}_{\substack{=Var[v(t-1)]\\=Var[v(t)]\\=\gamma(0)}}
		+\underbrace{E[\eta(t)^2]}_{=\lambda^2}
		+2a \underbrace{E[v(t-1)\eta(t)]}_{
					\substack{	v(t-1)\text{ depends on } \eta(t-2)\\
								\eta(t)\text{ independent of }\eta(t-2)\\
								\implies\\
								E[v(t-1)\eta(t)]=0\\
								}}\\
\gamma(0)&=a^2\gamma(0)+\lambda^2\\
\Aboxed{
\gamma(0)&=\frac{\lambda^2}{1-a^2}
}
\end{align*}
To find $\gamma(\tau)$, we start from the model $v(t)=av(t-1)+\eta(t)$.
\begin{alignat*}{4}
v(t)&=av(t-1)&+&\eta(t)\\
v(t)v(t-\tau)&=av(t-1)v(t-\tau)&+&\eta(t)v(t-\tau)\\
\underbrace{E[v(t)v(t-\tau)]}_{\gamma(\tau)}&=
	a\underbrace{E[v(t-1)v(t-\tau)]}_{\gamma(\tau-1)}&+&
	\underbrace{E[\eta(t)v(t-\tau)]}_0\\
\Aboxed{
\gamma(\tau)&=a\gamma(\tau-1)
}
\end{alignat*}
We can join the two by inductive reasoning, obtaining
\[\boxed{
\gamma(\tau)=a^{|\tau|}\frac{\lambda^2}{1-a^2}
}\]
\subparagraph{Long Division} Leads to same result, but is boring

\subsection{AR(n)}
\paragraph{Model}
\[
v(t)=a_1v(t-1)+a_2v(t-2)+...+a_nv(t-n)+\eta(t)
\]
\paragraph{Transfer function}
\[
W(z)=\frac{z^n}{z^n-a_1z_{n-1}-...-a_n}
\]
\paragraph{Mean}
\begin{alignat*}{6}
E[v(t)]	&=	a_1E[v(t-1])	&+&a_2E[v(t-2)]	&+&...	&+&a_nE[v(t-n)]	&+&\underbrace{E[\eta(t)]}_0\\
m		&=	a_1m			&+&a_2m			&+&...	&+&a_nm\\
(1-a_1-a_2-...-a_n)m&=0\\
\Aboxed{
E[v(t)]&=0
}
\end{alignat*}
\section{ARMA Processes}
\paragraph{Model}
\[
v(t)=
	a_1v(t-1)+...+a_{n_a}v(t-n_a)+
	c_0\eta(t)+...+c_{n_c}v(t-n_c)
\]
Can also be espressed as $V(t)=\frac{C(z)}{A(z)}\eta(t)$, where
\begin{align*}
C(z)&=c_0+c_1z^{-1}+...+c_{n_c}z^{-n_c}\\
A(z)&=1-a_1z^{-1}-...-a_{n_a}z^{-n_a}
\end{align*}
Such process is stationary if all the poles of $W(z)$ are inside the unit disk.

\section{Prediction problem}
We want to predict $v(t+r)$ from $v(t), v(t-1), ...$, where $r$ is called prediction horizon, of the following stationary process:

\tikzstyle{block}=[draw, minimum size=3.5em]

\begin{tikzpicture}[node distance=5em,auto,>=latex']
    \node [block] (W) {$W(z)$};
    \node (wn) [left of=W, coordinate] {};
    \node [coordinate] (v) [right of=W]{};
    \path[->] (wn) edge node {$\eta$} (W);
    \path[->] (W) edge node {$v$} (v);
\end{tikzpicture}

\subsection{Fake problem}
Having a process with transfer function $W(z)$, we can compute it in polynomial form using the long division algorithm
\[
W(z)=w_0+w_1z^{-1}+w_2z^{-2}+...
\]
We can calculate
\[
v(t+r)=W(z)\eta(t+r)
= \underbrace{w_0 \eta(t+r)+w_1 \eta(t+r-1)+...+w_{r-1} \eta(t+1)}_{\alpha(t)\textbf{ unpredictable: }\text{future of }\eta\text{ involved}}
+ \underbrace{w_r \eta(t)+w_{r+1} \eta(t-1)+...}_{\beta(t) \textbf{ predictable}}
\]
The optimal fake predictor is then
\[
\boxed{
v(t+r|t)=w_r \eta(t)+w_{r+1}\eta(t-1)+...
}=\beta(t)
\]
And the prediction error is
\begin{align*}
\epsilon(t)&=v(t+r)&-&\hat{v}(t+r|t)\\
&= \alpha(t)+\beta(t)&-&\beta(t)\\
&=\alpha(t)\\
\end{align*}
\[
\boxed{
\epsilon(t)=w_0 \eta(t+r)+w_1 \eta(t+r-1)+...+w_{r-1} \eta(t+1)
}
\]
\[
\boxed{
Var[\epsilon(t)]=(w_0^2+w_1^2+...+w_{r-1}^2)\lambda^2
}
\]
\subsection{True Problem}
We want to estimate $v(t+r)$ form $v(t)$, having transfer function $W(z)$ and $\hat{W}_r(z)$ the solution to the fake problem. We can calculate the transfer function of the real predictor from the process as 
\[
\boxed{
W_r(z)=W(z)^{-1} \cdot \hat{W}_r(z)
}
\]
For ARMA processes a shortcut exists:
\[
\hat{v}_{\text{ARMA}}(t|t-1)=\frac{C(z)A(z)}{C(z)}
\qquad \text{having } W(z)=\frac{C(z)}{A(z)}
\]
\subsection{Prediction with eXogenous variables}
An exogenous variable is a \underline{deterministic} input variable in the system
\subsubsection{ARX model}
\begin{align*}
v(t)&=a_1v(t-1)+...+a_{n_a}v(t-n_a)+b_1u(t-1)+...+b_{n_b}u(t-n_b)+\eta(t)
A(z)v(t)&=B(z)u(t-1)+\eta(t)
\end{align*}
\paragraph{Transfer functions from u and $\eta$}
\[
W_u(z)=\frac{B(z)}{A(z)}\\
W_{\eta}(z)=\frac{1}{A(z)}
\]
\subsubsection{ARMAX model}
\begin{align*}
A(z)v(t)&=C(z)\eta(t)+B(z)u(t-1)\\
y(t)&=W(z)\eta(t)+G(z)u(t)
\end{align*}
\paragraph{Predictor}
\[
\hat{y}(t|t-1)=
\frac{C(z)-A(z)}{C(z)}y(t)
+\frac{B(z)}{C(z)}u(t-1)
\]

\newpage
\part{Identification}
Consists of estimating a model from data.
\section{Prediction Error Minimization} Aims to minimize $\epsilon(t)=v(t)-\hat{v}(t|t-r)$\\
Steps:
\begin{enumerate}
\item \textbf{Data collection:} collect $\vec{u}$ and $\vec{y}$
\item \textbf{Family selection:} choose a family of models $M(\theta)$
	\begin{description}
	\item[MA(1)] $\theta=[a]$
	\item[MA(n)] $\theta=[a_1,...,a_n]$
	\item[ARMA($n_a,n_c$)] $\theta=[a_1,...,a_{n_a},
	c_1,...,c_{n_c}]$
	\item[...]
	\end{description}
\item \textbf{Select an optimization criterion}
	\begin{description}
	\item[Mean Squared error] $J(\theta)=\frac{1}{N}\sum_{t=1}^N \epsilon_\theta(t)^2$
	\item[Mean absolute error] $J(\theta)=\frac{1}{N}\sum_{t=1}^N |\epsilon_\theta(t)|$
	\item[...]
	\end{description}
\item  \textbf{Optimization} find $\hat{\theta}=argmin\,J(\theta)\implies \frac{dJ(\theta)}{d\theta}=0$
\item \textbf{Validation} verify if the result satisfies the requirements
\end{enumerate}

\newpage
\appendix
\part{Cheatsheet}
\section{Probability Recall}
\paragraph{Cross-Variance}
$Var[v,u]=E[(v-E[v])(u-E[u])]$
\paragraph{Variance Matrix}
$\begin{vmatrix}
	Var[v_1]			&	.	&	.	&	Var[v_1,v_k]		\\
		.			&	.	&		&		.			\\
		.			&		&	.	&		.			\\
	Var[v_k,v_1]		&	.	&	.	&	Var[v_k]			\\
				
\end{vmatrix}$
\paragraph{Covariance coefficient}
	$\delta[i,j]=\frac{Var[i,j]}{\sqrt{Var[i]}\sqrt{Var[j]}}$
\paragraph{Stationary process}
\begin{itemize}
	\item $m$ constant
	\item $\lambda^2$ constant
	\item covariance $\gamma(\tau)$ depends only on time difference
	\item $|\gamma(\tau)|\leq\gamma(0)		\quad\forall\tau$
\end{itemize}
\paragraph{White noise} $\eta(t)\sim WN(m,\lambda^2)$
\begin{itemize}
	\item Stationary process
	\item $\gamma(\tau)=0	\quad\forall\tau\neq0$
	\item $v(t)=\alpha \eta (t)+\beta\implies v(t)\sim WN(\beta,\alpha^2 \lambda^2)$
\end{itemize}
\paragraph{Canonical representation}
\begin{itemize}
\item Monic
\item Same degree
\item Coprime
\item Poles and zeros in unit disk
\end{itemize}
\section{Spectral analysis}
\paragraph{Spectrum}
\begin{itemize}
\item $\Gamma(\omega)=\gamma(0)+2cos(\omega)\gamma(1)+2cos(2\omega)\gamma(2)+...$
\item Periodic $T=2\pi$
\item Even
\item $\Gamma_\eta(\omega)=\gamma_\eta(0)=\lambda^2$
\end{itemize}
\paragraph{Complex spectrum}
\begin{itemize}
\item $\Phi(z)=\sum_{\tau =-\infty}^{+\infty} \omega(\tau)z^{-\tau}$
\item $\Gamma(\omega)=\Phi(e^{j\omega})$
\end{itemize}
\paragraph{Fundamental theorem of spectral analysis}
\begin{itemize}
\item $\Gamma_{\text{out}}(\omega)=|W(e^{j\omega})|^2 \cdot \Gamma_{\text{in}}(\omega)$
\item $\Phi_{\text{out}}(z)=W(z)W(z^{-1}) \cdot \Phi_{\text{in}}(z)$
\end{itemize}
\section{Moving Average MA(n)}
\begin{itemize}
\item $W(z)=\frac{c_0z^n+c_1z^{n-1}+...+c_n}{z_n}$
\item $m=0$
\item $\gamma(\tau)= 
\begin{cases}
(c_0c_\tau+c_1c_{1+\tau}+...+c_{n-\tau}c_\tau)\lambda^2	&	|\tau|\leq n\\
0	& \text{otherwise}
\end{cases}$
\end{itemize}
\subsection{MA($\infty$)}
\begin{itemize}
\item $\gamma(0)=(c_0^2+c_1^2+...+c_k^2+...)\lambda^2$
\item $\gamma(0)$ must converge to a finite value
\end{itemize}
\section{Auto Regressive AR(n)}
\begin{itemize}
\item $m=0$
\item $W(z)=\frac{z^n}{z^n-a_1z_{n-1}-...-a_n}$
\item Covariance calculated by its definition
\end{itemize}

\section{Known predictors}
\begin{description}
\item[AR(1)] $\hat{v}(t|t-r)=a^rv(t-r)$
\item[MA(1)] $\hat{v}(t|t-1)=v(t-1)-c\hat{v}(t-1|t-2)$
\item[MA(n)] $\hat{v}(t|t-\textbf{k})=0 \quad \forall k>n$
\item[ARMA($n_a,n_b$)] $\hat{v}(t|t-1)=
\frac{C(z)-A(z)}{C(z)}v(t)$
\item[ARMAX($n_a,n_b$)] $\hat{y}(t|t-1)=
\frac{C(z)-A(z)}{C(z)}y(t)
+\frac{B(z)}{C(z)}u(t-1)$
\end{description}
\end{document}
